#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Táº£i song song cÃ¡c file .pbf tá»« HAR (tá»‘i Ä‘a 10 luá»“ng).
Tá»± Ä‘á»™ng retry, táº¡o thÆ° má»¥c z/x/y.pbf Ä‘Ãºng chuáº©n.
"""

import os, json, sys, time, random, requests
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==== Cáº¤U HÃŒNH ====
MAX_WORKERS = 10          # sá»‘ luá»“ng song song tá»‘i Ä‘a
MAX_RETRIES = 5           # sá»‘ láº§n thá»­ láº¡i
BACKOFF_FACTOR = 1.8      # há»‡ sá»‘ tÄƒng delay khi retry
SLEEP_BETWEEN = 0.05      # delay nhá» giá»¯a cÃ¡c lÆ°á»£t táº£i
TIMEOUT = 30              # timeout má»—i request

# User-Agent (tá»‘t Ä‘á»ƒ set)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36",
    "Referer": "https://maps.goong.io/",
    "Origin": "https://maps.goong.io/"
}

# ===================

def download_file(url, out_path):
    """Táº£i 1 file cÃ³ retry logic"""
    if os.path.exists(out_path) and os.path.getsize(out_path) > 0:
        return True

    retry, delay = 0, SLEEP_BETWEEN
    while retry <= MAX_RETRIES:
        try:
            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            if r.status_code == 200:
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                with open(out_path, "wb") as f:
                    f.write(r.content)
                return True
            elif r.status_code in (429, 500, 502, 503, 504):
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                retry += 1
            else:
                print(f"[WARN] HTTP {r.status_code} {url}")
                return False
        except Exception as e:
            retry += 1
            time.sleep(delay)
            delay *= BACKOFF_FACTOR
    return False


def extract_pbf_urls(har_path):
    with open(har_path, "r", encoding="utf-8") as f:
        har = json.load(f)
    entries = har.get("log", {}).get("entries", [])
    urls = []
    for e in entries:
        url = e.get("request", {}).get("url", "")
        if url.endswith(".pbf") or ".pbf?" in url:
            urls.append(url)
    return urls


def url_to_path(url, out_dir):
    parsed = urlparse(url)
    parts = parsed.path.strip("/").split("/")
    out_path = os.path.join(out_dir, *parts[-4:])
    return out_path


def main():
    if len(sys.argv) < 2:
        print("Usage: python3 download_from_har_concurrent.py <file.har> [out_dir]")
        sys.exit(1)

    har_path = sys.argv[1]
    out_dir = sys.argv[2] if len(sys.argv) >= 3 else "tiles_from_har"

    os.makedirs(out_dir, exist_ok=True)
    urls = extract_pbf_urls(har_path)

    print(f"ğŸ“¦ Tá»•ng request trong HAR: {len(urls)} file .pbf Ä‘Æ°á»£c tÃ¬m tháº¥y.")
    print(f"ğŸš€ Báº¯t Ä‘áº§u táº£i song song tá»‘i Ä‘a {MAX_WORKERS} luá»“ng...\n")

    downloaded = 0
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {}
        for url in urls:
            out_path = url_to_path(url, out_dir)
            futures[executor.submit(download_file, url, out_path)] = url

        for i, future in enumerate(as_completed(futures), start=1):
            url = futures[future]
            try:
                result = future.result()
                if result:
                    downloaded += 1
                    print(f"[{i}/{len(urls)}] âœ… OK: {url}")
                else:
                    print(f"[{i}/{len(urls)}] âŒ FAIL: {url}")
            except Exception as e:
                print(f"[{i}/{len(urls)}] ğŸ’¥ ERROR {e} @ {url}")
            time.sleep(random.uniform(0, 0.02))

    print(f"\nâœ… HoÃ n táº¥t: {downloaded}/{len(urls)} file táº£i thÃ nh cÃ´ng.")
    print(f"ğŸ“ LÆ°u táº¡i: {os.path.abspath(out_dir)}")


if __name__ == "__main__":
    main()
